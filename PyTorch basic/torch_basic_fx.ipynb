{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://www.jianshu.com/p/d678c5e44a6b\n",
    "torch常用基础函数    \n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F     # 激励函数都在这\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "# python 的可视化模块, 我有教程 (https://morvanzhou.github.io/tutorials/data-manipulation/plt/)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.张量Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0658, 0.4588, 0.3396],\n",
       "        [0.8218, 0.6243, 0.2209],\n",
       "        [0.9032, 0.3301, 0.4968],\n",
       "        [0.5803, 0.2985, 0.0403],\n",
       "        [0.2270, 0.4331, 0.0696]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果obj是一个pytorch张量，则返回True\n",
    "torch.is_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果obj是一个pytorch storage对象，则返回True\n",
    "torch.is_storage(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回input张量中的元素个数\n",
    "torch.numel(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 创建操作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.eye(n, m=None, out=None):\n",
    "返回一个2维张量，对角线为1,其它位置为0\n",
    "\n",
    "n (int) -行数\n",
    "m (int, optional)列数，如果为None,则默认为n\n",
    "out (Tensor, optional)\n",
    "\"\"\"\n",
    "torch.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nda = np.array([[1, 2], [3, 4]])\n",
    "print(type(nda))\n",
    "nda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nda_tr = torch.from_numpy(nda)\n",
    "nda_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n将numpy.ndarray转换为Tensor，返回的张量tensor和numpy的ndarray共享同一内存空间，\\n修改一个会导致另一个也被修改，返回的张量不能改变大小\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "将numpy.ndarray转换为Tensor，返回的张量tensor和numpy的ndarray共享同一内存空间，\n",
    "修改一个会导致另一个也被修改，返回的张量不能改变大小\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 9],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nda[0,1]=9\n",
    "nda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 9],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nda_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.linspace(start, end, steps=100, out=None):\\n返回一个1维张量，包含在start和end上均匀间隔的steps个点\\n\\nstart (float) -序列起点\\nend (float) - 序列终点\\nsteps (int) - 在start与end间生成的样本数\\nout (Tensor, optional) - 结果张量\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.linspace(start, end, steps=100, out=None):\n",
    "返回一个1维张量，包含在start和end上均匀间隔的steps个点\n",
    "\n",
    "start (float) -序列起点\n",
    "end (float) - 序列终点\n",
    "steps (int) - 在start与end间生成的样本数\n",
    "out (Tensor, optional) - 结果张量\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.,  12.,  23.,  34.,  45.,  56.,  67.,  78.,  89., 100.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1, 100, steps=10,out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.logspace(start, end, steps=100, out=None):\\n返回一个1维张量，包含在区间10exp(start)和10exp(end)上以对数刻度均匀间隔的 steps个点。\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.logspace(start, end, steps=100, out=None):\n",
    "返回一个1维张量，包含在区间10exp(start)和10exp(end)上以对数刻度均匀间隔的 steps个点。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0000e+02, 1.0000e+04, 1.0000e+06, 1.0000e+08, 1.0000e+10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logspace(0, 10, steps=6,out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.ones(*sizes, out=None):\\n返回一个全为1的张量，形状由可变参数sizes定义\\n\\nsizes (int...) - 整数序列，定义了输出形状\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.ones(*sizes, out=None):\n",
    "返回一个全为1的张量，形状由可变参数sizes定义\n",
    "\n",
    "sizes (int...) - 整数序列，定义了输出形状\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,4, out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.rand(*sizes, out=None):\\n返回一个张量，包含了从区间(0, 1)的均匀分布中抽取的一组随机数，形状由可变参数sizes定义。\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.rand(*sizes, out=None):\n",
    "返回一个张量，包含了从区间(0, 1)的均匀分布中抽取的一组随机数，形状由可变参数sizes定义。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6949, 0.6991, 0.9297, 0.9939, 0.2891],\n",
       "        [0.1115, 0.2001, 0.9080, 0.6309, 0.8513],\n",
       "        [0.0759, 0.7766, 0.9298, 0.2397, 0.5414]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.randn(*sizes, out=None):\\n返回一个张量，包含了从标准正态分布(mean=0, std=1)中抽取一组随机数，形状由可变参数sizes定义。\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.randn(*sizes, out=None):\n",
    "返回一个张量，包含了从标准正态分布(mean=0, std=1)中抽取一组随机数，形状由可变参数sizes定义。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3979, -0.4411,  0.5089, -0.0359],\n",
       "        [ 0.8834,  0.6152,  1.1017, -0.2618],\n",
       "        [ 1.0798, -1.1068,  0.9518, -0.7999]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.randperm(n, out=None):\\n给定参数n，返回一个从0到n-1的随机整数排列\\n\\nn (int) - 上边界(不包含）\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.randperm(n, out=None):\n",
    "给定参数n，返回一个从0到n-1的随机整数排列\n",
    "\n",
    "n (int) - 上边界(不包含）\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 8, 3, 2, 0, 4, 6, 7, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.arange(start, end, step=1, out=None):\\n返回一个1维张量，长度为floor((end-start)/step)，以step`为步长的一组序列值。\\n\\nstart (float) - 起点\\nend (float) - 终点(不包含）\\nstep (float) - 相邻点的间隔大小\\nout (Tensor, optional)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.arange(start, end, step=1, out=None):\n",
    "返回一个1维张量，长度为floor((end-start)/step)，以step`为步长的一组序列值。\n",
    "\n",
    "start (float) - 起点\n",
    "end (float) - 终点(不包含）\n",
    "step (float) - 相邻点的间隔大小\n",
    "out (Tensor, optional)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 7])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10, step=3, out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jcd/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 9.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.range(1, 10, step=4, out=None)\n",
    "# 推荐使用torch.arange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.zeros(*sizes, out=None):\\n返回一个全为标量0的张量，形状由可变参数sizes定义\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.zeros(*sizes, out=None):\n",
    "返回一个全为标量0的张量，形状由可变参数sizes定义\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 索引，切片，连接，换位(Index, Slicing, Joining, Mutating)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.cat(inputs, dimension=0):\\n在给定维度上对输入的张量序列seq进行连接操作。\\n\\ninputs (sequence of Tensors)\\ndimension (int optional) - 沿着此维连接张量序列\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.cat(inputs, dimension=0):\n",
    "在给定维度上对输入的张量序列seq进行连接操作。\n",
    "\n",
    "inputs (sequence of Tensors)\n",
    "dimension (int optional) - 沿着此维连接张量序列\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0658, 0.4588, 0.3396],\n",
       "        [0.8218, 0.6243, 0.2209],\n",
       "        [0.9032, 0.3301, 0.4968],\n",
       "        [0.5803, 0.2985, 0.0403],\n",
       "        [0.2270, 0.4331, 0.0696]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0658, 0.4588, 0.3396, 0.0658, 0.4588, 0.3396, 0.0658, 0.4588, 0.3396],\n",
       "        [0.8218, 0.6243, 0.2209, 0.8218, 0.6243, 0.2209, 0.8218, 0.6243, 0.2209],\n",
       "        [0.9032, 0.3301, 0.4968, 0.9032, 0.3301, 0.4968, 0.9032, 0.3301, 0.4968],\n",
       "        [0.5803, 0.2985, 0.0403, 0.5803, 0.2985, 0.0403, 0.5803, 0.2985, 0.0403],\n",
       "        [0.2270, 0.4331, 0.0696, 0.2270, 0.4331, 0.0696, 0.2270, 0.4331, 0.0696]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0658, 0.4588, 0.3396],\n",
       "        [0.8218, 0.6243, 0.2209],\n",
       "        [0.9032, 0.3301, 0.4968],\n",
       "        [0.5803, 0.2985, 0.0403],\n",
       "        [0.2270, 0.4331, 0.0696],\n",
       "        [0.0658, 0.4588, 0.3396],\n",
       "        [0.8218, 0.6243, 0.2209],\n",
       "        [0.9032, 0.3301, 0.4968],\n",
       "        [0.5803, 0.2985, 0.0403],\n",
       "        [0.2270, 0.4331, 0.0696],\n",
       "        [0.0658, 0.4588, 0.3396],\n",
       "        [0.8218, 0.6243, 0.2209],\n",
       "        [0.9032, 0.3301, 0.4968],\n",
       "        [0.5803, 0.2985, 0.0403],\n",
       "        [0.2270, 0.4331, 0.0696]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.chunk(tensor, chunks, dim=0):\\n在给定维度上将输入张量进行分块\\n\\ntensors(Tensors) - 待分场的输入张量\\nchunks (int) - 分块的个数\\ndim (int) - 沿着此维度\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.chunk(tensor, chunks, dim=0):\n",
    "在给定维度上将输入张量进行分块\n",
    "\n",
    "tensors(Tensors) - 待分块的输入张量\n",
    "chunks (int) - 分块的个数\n",
    "dim (int) - 沿着此维度\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0658, 0.4588],\n",
       "         [0.8218, 0.6243],\n",
       "         [0.9032, 0.3301],\n",
       "         [0.5803, 0.2985],\n",
       "         [0.2270, 0.4331]]), tensor([[0.3396],\n",
       "         [0.2209],\n",
       "         [0.4968],\n",
       "         [0.0403],\n",
       "         [0.0696]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(x, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.gather(input, dim, index, out=None):\\n沿给定轴dim,将输入索引张量index指定位置的值进行聚合。\\n\\ninput(Tensor) - 源张量\\ndim(int) - 索引的轴\\nindex(LongTensor) - 聚合元素的下标\\nout - 目标张量\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.gather(input, dim, index, out=None):\n",
    "沿给定轴dim,将输入索引张量index指定位置的值进行聚合。\n",
    "\n",
    "input(Tensor) - 源张量\n",
    "dim(int) - 索引的轴\n",
    "index(LongTensor) - 聚合元素的下标\n",
    "out - 目标张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0658, 0.4588, 0.3396],\n",
       "        [0.8218, 0.6243, 0.2209],\n",
       "        [0.9032, 0.3301, 0.4968],\n",
       "        [0.5803, 0.2985, 0.0403],\n",
       "        [0.2270, 0.4331, 0.0696]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0658, 0.4588],\n",
       "        [0.6243, 0.6243],\n",
       "        [0.4968, 0.4968],\n",
       "        [0.5803, 0.5803],\n",
       "        [0.0696, 0.4331]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(x, 1, torch.LongTensor([[0,1],[1,1],[2,2],[0,0],[2,1]]))\n",
    "# 即表示取第一行的index 0,1 第二行的index 1,1 第三行的index 2,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.index_select(input, dim, index, out=None):\\n沿指定维度对输入进行切片，取index中指定的相应项，然后返回一个新的张量，返回的张量与原始张量有相同的维度(在指定轴上)，返回的张量与原始张量不共享内存空间\\n\\ninput(Tensor) - 输入张量\\ndim(int) - 索引的轴\\nindex(LongTensor) - 包含索引下标的一维张量\\nout - 目标张量\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.index_select(input, dim, index, out=None):\n",
    "沿指定维度对输入进行切片，取index中指定的相应项，然后返回一个新的张量，返回的张量与原始张量有相同的维度(在指定轴上)，返回的张量与原始张量不共享内存空间\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "dim(int) - 索引的轴\n",
    "index(LongTensor) - 包含索引下标的一维张量\n",
    "out - 目标张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0658, 0.4588, 0.3396],\n",
       "        [0.9032, 0.3301, 0.4968]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(x, 0, torch.tensor([0,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.masked_select(input, mask, out=None):\\n根据掩码张量mask中的二元值，取输入张量中的指定项，将取值返回到一个新的1D张量。\\n张量mask须跟input张量有相同的元素数目，但形状或维度不需要相同。返回的张量不与原始张量共享内存空间\\n\\ninput(Tensor) - 输入张量\\nmask(ByteTensor) - 掩码张量，包含了二元索引值\\nout - 目标张量\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.masked_select(input, mask, out=None):\n",
    "根据掩码张量mask中的二元值，取输入张量中的指定项，将取值返回到一个新的1D张量。\n",
    "张量mask须跟input张量有相同的元素数目，但形状或维度不需要相同。返回的张量不与原始张量共享内存空间\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "mask(ByteTensor) - 掩码张量，包含了二元索引值\n",
    "out - 目标张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0658, 0.4588, 0.3396],\n",
       "        [0.8218, 0.6243, 0.2209],\n",
       "        [0.9032, 0.3301, 0.4968],\n",
       "        [0.5803, 0.2985, 0.0403],\n",
       "        [0.2270, 0.4331, 0.0696]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/LegacyDefinitions.cpp:67: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0658, 0.4588, 0.8218, 0.6243, 0.9032, 0.3301, 0.0403, 0.2270, 0.0696])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ByteTensor([[1,1,0],[1,1,0],[1,1,0], [0,0,1], [1,0,1]])\n",
    "torch.masked_select(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.nonzero(input, out=None):\\n返回一个包含输入input中非零元素索引的张量，输出张量中的每行包含输入中非零元素的索引\\n若输入input有n维，则输出的索引张量output形状为z * n, 这里z是输入张量input中所有非零元素的个数\\n\\ninput(Tensor) - 输入张量\\nout - 包含索引值的结果张量\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.nonzero(input, out=None):\n",
    "返回一个包含输入input中非零元素索引的张量，输出张量中的每行包含输入中非零元素的索引\n",
    "若输入input有n维，则输出的索引张量output形状为z * n, 这里z是输入张量input中所有非零元素的个数\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "out - 包含索引值的结果张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 1],\n",
       "        [0, 2],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [2, 0],\n",
       "        [2, 1],\n",
       "        [2, 2],\n",
       "        [3, 0],\n",
       "        [3, 1],\n",
       "        [3, 2],\n",
       "        [4, 0],\n",
       "        [4, 1],\n",
       "        [4, 2]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.split(tensor, split_size, dim=0):\\n将输入张量分割成相等形状的chunks(如果可分)。如果沿指定维的张量形状大小不能被split_size整分，则最后一个分块会小于其它分块。\\n\\ntensor(Tensor) - 待分割张量\\nsplit_size(int) - 单个分块的形状大小\\ndim(int) - 沿着此维进行分割\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.split(tensor, split_size, dim=0):\n",
    "将输入张量分割成相等形状的chunks(如果可分)。如果沿指定维的张量形状大小不能被split_size整分，则最后一个分块会小于其它分块。\n",
    "\n",
    "tensor(Tensor) - 待分割张量\n",
    "split_size(int) - 单个分块的形状大小\n",
    "dim(int) - 沿着此维进行分割\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0658, 0.4588, 0.3396],\n",
       "         [0.8218, 0.6243, 0.2209],\n",
       "         [0.9032, 0.3301, 0.4968],\n",
       "         [0.5803, 0.2985, 0.0403]]), tensor([[0.2270, 0.4331, 0.0696]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(x, 4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.squeeze(input, dim=None, out=None):\n",
    "# 将输入张量形状中的1去除并返回，如果输入是形如(A * 1 * B * 1 * C * 1 *D)，那么输出形状就为：(A * B * C * D)。\n",
    "# 当给定dim时，则只在给定维度上进行挤压，如输入形状为(A * 1 * B)，squeeze(input, 0)，将会保持张量不变，只有用squeeze(input, 1)，形状会变成(A *B)。\n",
    "# 输入张量与返回张量共享内存\n",
    "\n",
    "# input(Tensor) - 输入张量\n",
    "# dim(int, optional) - 如果给定，则只在给定维度挤压\n",
    "# out(Tensor, optional) - 输出张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(2, 1, 2, 1, 2)\n",
    "torch.squeeze(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.stack(sequence, dim=0):\\n沿着一个新维度对输入张量进行连接，序列中所有张量都应该为相同的形状。\\n\\nsequence(Sequence) - 待连接的张量序列\\ndim(int) - 插入的维度\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.stack(sequence, dim=0):\n",
    "沿着一个新维度对输入张量进行连接，序列中所有张量都应该为相同的形状。\n",
    "\n",
    "sequence(Sequence) - 待连接的张量序列\n",
    "dim(int) - 插入的维度\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0658, 0.4588, 0.3396],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.8218, 0.6243, 0.2209],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.9032, 0.3301, 0.4968],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.5803, 0.2985, 0.0403],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.2270, 0.4331, 0.0696],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(5,3)\n",
    "z = torch.ones(5,3)\n",
    "torch.stack([x, y, z], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.t(input, out=None):\\n输入一个矩阵(2维张量)，并转置0,1维，可以被视为transpose(input, 0, 1)的简写函数\\n\\ninput(Tensor) - 输入张量\\nout(Tensor, optional) - 结果张量\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.t(input, out=None):\n",
    "输入一个矩阵(2维张量)，并转置0,1维，可以被视为transpose(input, 0, 1)的简写函数\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "out(Tensor, optional) - 结果张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0658, 0.4588, 0.3396],\n",
       "        [0.8218, 0.6243, 0.2209],\n",
       "        [0.9032, 0.3301, 0.4968],\n",
       "        [0.5803, 0.2985, 0.0403],\n",
       "        [0.2270, 0.4331, 0.0696]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0658, 0.8218, 0.9032, 0.5803, 0.2270],\n",
       "        [0.4588, 0.6243, 0.3301, 0.2985, 0.4331],\n",
       "        [0.3396, 0.2209, 0.4968, 0.0403, 0.0696]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.transpose(input, dim0, dim1, out=None):\\n返回输入矩阵input的转置，交换维度dim0和dim1。输入张量与输出张量共享内存。\\n\\ninput(Tensor) - 输入张量\\ndim0(int) - 转置的第一维\\ndim1(int) - 转置的第二维\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.transpose(input, dim0, dim1, out=None):\n",
    "返回输入矩阵input的转置，交换维度dim0和dim1。输入张量与输出张量共享内存。\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "dim0(int) - 转置的第一维\n",
    "dim1(int) - 转置的第二维\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0658, 0.8218, 0.9032, 0.5803, 0.2270],\n",
       "        [0.4588, 0.6243, 0.3301, 0.2985, 0.4331],\n",
       "        [0.3396, 0.2209, 0.4968, 0.0403, 0.0696]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(x, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.unbind(tensor, dim=0)[source]:\\n移除指定维度后，返回一个元组，包含了沿着指定维切片后的各个切片\\n\\ntensor(Tensor) - 输入张量\\ndim(int) - 删除的维度\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.unbind(tensor, dim=0)[source]:\n",
    "移除指定维度后，返回一个元组，包含了沿着指定维切片后的各个切片\n",
    "\n",
    "tensor(Tensor) - 输入张量\n",
    "dim(int) - 删除的维度\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0658, 0.4588, 0.3396]),\n",
       " tensor([0.8218, 0.6243, 0.2209]),\n",
       " tensor([0.9032, 0.3301, 0.4968]),\n",
       " tensor([0.5803, 0.2985, 0.0403]),\n",
       " tensor([0.2270, 0.4331, 0.0696]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0658, 0.8218, 0.9032, 0.5803, 0.2270]),\n",
       " tensor([0.4588, 0.6243, 0.3301, 0.2985, 0.4331]),\n",
       " tensor([0.3396, 0.2209, 0.4968, 0.0403, 0.0696]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.unsqueeze(input, dim, out=None):\\n返回一个新的张量，对输入的指定位置插入维度1，返回张量与输入张量共享内存，若dim为负，则将被转化为dim+input.dim()+1\\n\\ntensor(Tensor) - 输入张量\\ndim(int) - 插入维度的索引\\nout(Tensor, optional) - 结果张量\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.unsqueeze(input, dim, out=None):\n",
    "返回一个新的张量，对输入的指定位置插入维度1，返回张量与输入张量共享内存，若dim为负，则将被转化为dim+input.dim()+1\n",
    "\n",
    "tensor(Tensor) - 输入张量\n",
    "dim(int) - 插入维度的索引\n",
    "out(Tensor, optional) - 结果张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0658, 0.4588, 0.3396]],\n",
       "\n",
       "        [[0.8218, 0.6243, 0.2209]],\n",
       "\n",
       "        [[0.9032, 0.3301, 0.4968]],\n",
       "\n",
       "        [[0.5803, 0.2985, 0.0403]],\n",
       "\n",
       "        [[0.2270, 0.4331, 0.0696]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(x, 1)\n",
    "# Shape: torch.Size([5, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0658, 0.4588, 0.3396],\n",
       "         [0.8218, 0.6243, 0.2209],\n",
       "         [0.9032, 0.3301, 0.4968],\n",
       "         [0.5803, 0.2985, 0.0403],\n",
       "         [0.2270, 0.4331, 0.0696]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(x, 0)\n",
    "# Shape: torch.Size([5, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(x, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(x, 2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 随机抽样Random sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(seed)\n",
    "# 设定生成随机数的种子，并返回一个torch._C.Generator对象\n",
    "# 参数: seed (int or long) – 种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12cc6e6b0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#返回生成随机数的原始种子值（python long）\n",
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.get_rng_state()[source]\n",
    "# 返回随机生成器状态(ByteTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([60,  0,  0,  ...,  0,  0,  0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng_state = torch.get_rng_state()\n",
    "rng_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5056"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rng_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_rng_state(new_state)[source]:\n",
    "# 设定随机生成器状态参数：new_state(torch.ByteTensor) - 期望的状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_rng_state(rng_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12cc6e6b0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.default_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.bernoulli(input, out=None):\n",
    "# 从伯努利分布中抽取二元随机数(0或者1），输入中所有值必须在[0, 1]区间，\n",
    "# 输出张量的第i个元素值，将以输入张量的第i个概率值等于1。\n",
    "# 返回值将会是与输入相同大小的张量，每个值为0或1\n",
    "\n",
    "# input(Tensor) - 输入为伯努利分布的概率值(probability of drawing \"1\")\n",
    "# out(Tensor, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0844, 0.8128, 0.3748],\n",
       "        [0.3646, 0.4697, 0.0277],\n",
       "        [0.5339, 0.8345, 0.8320]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor(3, 3).uniform_(0, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 1., 1.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bernoulli(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3395, 0.4570, 0.3396],\n",
       "        [0.8218, 0.6243, 0.2209],\n",
       "        [0.9032, 0.3301, 0.4968],\n",
       "        [0.5803, 0.2985, 0.0403],\n",
       "        [0.2270, 0.4331, 0.0696]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0]=0.3395\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.multinomial(input, num_samples, replacement=False, out=None):\n",
    "# 返回一个张量，每行包含从input相应行中定义的多项式分布中抽取的num_samples个样本。\n",
    "# input每行的值不需要总和为1,但必须非负且总和不能为0.\n",
    "\n",
    "# 当抽取样本时，依次从左到右排列(第一个样本对应第一列)。\n",
    "\n",
    "# 如果输入input是一个向量，输出out也是一个相同长度num_samples的向量。如果输入input是有 m行的矩阵，输出out是形如m×n的矩阵。\n",
    "\n",
    "# 如果参数replacement 为 True, 则样本抽取可以重复。否则，一个样本在每行不能被重复抽取。\n",
    "\n",
    "# 参数num_samples必须小于input长度(即，input的列数，如果是input是一个矩阵)。\n",
    "\n",
    "# input(Tensor) - 包含概率值的张量\n",
    "# num_samples(int) - 抽取的样本数\n",
    "# replacement(bool, optional) - 布尔值，决定是否能重复抽取\n",
    "# out(Tensor, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0],\n",
       "        [1, 0],\n",
       "        [0, 2],\n",
       "        [2, 0],\n",
       "        [1, 2]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(x, 2)\n",
    "# 概率越高越容易选中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.normal(means, std, out=None):\n",
    "# 返回一个张量，包含从给定means, std的离散正态分布中抽取随机数，均值和标准差的形状不须匹配，\n",
    "# 但每个张量的元素个数须相同\n",
    "\n",
    "# means(Tensor) - 均值\n",
    "# std(Tensor) - 标准差\n",
    "# out(Tensor, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data = torch.ones(5, 2)\n",
    "n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7864, 2.7545],\n",
       "        [1.5395, 3.7179],\n",
       "        [3.1939, 1.4783],\n",
       "        [2.3638, 2.4317],\n",
       "        [1.5872, 1.8699]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.normal(2 * n_data, 1)\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0051, 2.4646, 2.2557, 2.0001]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(2, 0.2, size=(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1., 11.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6073, 2.2569, 3.1642, 5.3245, 5.4841, 6.4559, 7.2141, 7.9992, 8.9548,\n",
       "        9.9119])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 序列化 Serialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(obj, f, pickle_module, pickle_protocol=2):\n",
    "# 保存一个对象到一个硬盘文件上\n",
    "\n",
    "# obj - 保存对象\n",
    "# f - 类文件对象\n",
    "# pickle_module - 用于pickling元数据和对象的模块\n",
    "# pickle_protocol - 指定pickle protocal可以覆盖默认参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x, 'tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(f, map_location=None, pickle_module=):\n",
    "# 从磁盘文件中读取一个通过torch.save()保存的对象，可通过参数map_location动态地进行内存重映射\n",
    "\n",
    "# f - 类文件对象\n",
    "# map_location - 一个函数或字典规定如何remap存储位置\n",
    "# pickle_module - 用于unpickling元数据和对象的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3023, 0.1208, 0.1272],\n",
       "        [0.1766, 0.5085, 0.8517],\n",
       "        [0.5513, 0.9506, 0.6067],\n",
       "        [0.8191, 0.4089, 0.0038],\n",
       "        [0.6152, 0.8712, 0.9641]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('tensor.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 并行化 Parallelism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获得用于并行化CPU操作的OpenMP线程数\n",
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设定用于并行化CPU操作的OpenMP线程数\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. 数学操作 Math operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.1 Pointwise Ops**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "torch.abs(input, out=None):\n",
    "计算输入张量的每个元素绝对值\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "out(Tensor, optional) - 结果张量\n",
    "\n",
    "torch.ceil(input, out=None):\n",
    "对输入input张量每个元素向上取整，即取不小于每个元素的最小整数，并返回结果到输出\n",
    "\n",
    "torch.exp(tensor, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的指数\n",
    "\n",
    "torch.floor(input, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的floor，即不大于元素的最大整数。\n",
    "\n",
    "torch.frac(tensor, out=None):\n",
    "返回每个元素的分数部分\n",
    "\n",
    "torch.log(input, out=None):\n",
    "计算input的自然对数\n",
    "\n",
    "torch.log1p(input, out=None):\n",
    "计算input + 1的自然对数y = log(x + 1)\n",
    "对值比较小的输入，此函数比torch.log()更准确\n",
    "\n",
    "torch.neg(input, out=None):\n",
    "返回一个新张量，包含输入input张量按元素取负。\n",
    "\n",
    "torch.reciprocal(input, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的倒数，即1.0/x\n",
    "\n",
    "torch.round(input, out=None):\n",
    "返回一个新张量，将输入input张量每个元素四舍五入到最近的整数。\n",
    "\n",
    "torch.rsqrt(input, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的平方根倒数。\n",
    "\n",
    "torch.sigmoid(input, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的sigmoid值\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(torch.tensor([-1, -3, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "torch.add(input, value, out=None):\n",
    "对输入张量input逐元素加上标量值value，并返回结果到一个新的张量。\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "value(Number) - 添加到输入每个元素的数\n",
    "out(Tensor, optional)\n",
    "\n",
    "torch.div(input, value, out=None):\n",
    "将input逐元素除以标量值value，并返回结果到输出张量out\n",
    "\n",
    "torch.mul(input, value, out=None):\n",
    "用标量值value乘以输入input的每个元素，并返回一个新的结果张量\n",
    "\n",
    "torch.mul(input, other, out=None):\n",
    "两个张量input, other按元素相乘，并返回到输出张量，两个张量形状不须匹配，但总元素数须一致。当形状不匹配时，input的形状作为输出张量的形状\n",
    "\n",
    "torch.fmod(input, divisor, out=None):\n",
    "计算除法余数，余数的正负与被除数相同\n",
    "\n",
    "torch.remainder(input, divisor, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的除法余数，余数与除数有相同的符号。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 1., 0., 1.])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None):\n",
    "用tensor2对tensor1逐元素相除，然后乘以标量值value并加到tensor上。\n",
    "\n",
    "tensor(Tensor) - 张量\n",
    "value(Number, optional) - 标量\n",
    "tensor1(Tensor) - 张量，作为分子\n",
    "tensor2(Tensor) - 张量，作为分母\n",
    "out(Tensor, optional)\n",
    "\n",
    "torch.addcmul(tensor, value=1, tensor1, tensor2, out=None):\n",
    "用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor，张量形状不需要匹配，但元素数量必须一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2000],\n",
       "         [-1.2097],\n",
       "         [ 0.4537]]), tensor([[2., 2., 2.]]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.zeros(3,3)\n",
    "t1 = torch.randn(3,1)\n",
    "t2 = torch.ones(1,3)*2\n",
    "t1,t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0100, -0.0100, -0.0100],\n",
       "        [-0.0605, -0.0605, -0.0605],\n",
       "        [ 0.0227,  0.0227,  0.0227]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.addcdiv(t, 0.1, t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.clamp(input, min, max, out=None):\n",
    "# 将输入input张量每个元素值约束到区间[min, max]，并返回结果到一个新张量\n",
    "# 也可以只设定min或只设定max\n",
    "\n",
    "# input(Tensor) - 输入张量\n",
    "# min(Number) - 限制范围下限\n",
    "# max(Number) - 限制范围上限\n",
    "# out(Tensor, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3395, 0.4570, 0.3396],\n",
       "        [0.8218, 0.6243, 0.2209],\n",
       "        [0.9032, 0.3301, 0.4968],\n",
       "        [0.5803, 0.2985, 0.0403],\n",
       "        [0.2270, 0.4331, 0.0696]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4000, 0.4570, 0.4000],\n",
       "        [0.6000, 0.6000, 0.4000],\n",
       "        [0.6000, 0.4000, 0.4968],\n",
       "        [0.5803, 0.4000, 0.4000],\n",
       "        [0.4000, 0.4331, 0.4000]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(x, 0.4, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.pow(input, exponent, out=None):\n",
    "# 对输入input按元素求exponent次幂，并返回结果张量。幂可以为float数或与input相同元素数的张量\n",
    "\n",
    "# torch.pow(base, input, out=None):\n",
    "# base为标量浮点值，input为张量。\n",
    "\n",
    "# base(float) - 标量值，指数的底\n",
    "# input(Tensor) - 幂值\n",
    "# out(Tensor, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  9, 27])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(3, torch.arange(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.lerp(start, end, weight, out=None):\n",
    "# 对两个张量以start, end做线性插值，将结果返回到输出张量\n",
    "# out = start + weight*(end - start)\n",
    "\n",
    "# start(Tensor) - 起始点张量\n",
    "# end(Tensor) - 终止点张量\n",
    "# weight(float) - 插值公式中的weight\n",
    "# out(Tensor, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = torch.arange(1., 5.)\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 5., 5., 5.])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end = torch.empty(4).fill_(5)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0000, 3.5000, 4.0000, 4.5000])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.5 是权重，代表start 和 end 的权重各占一半\n",
    "torch.lerp(start, end, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2 Reduction Ops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.dist(input, other, p=2, out=None) -> Tensor:\n",
    "# 返回(input - other)的p范数\n",
    "\n",
    "# input(Tensor) - 输入张量\n",
    "# other(Tensor) - 右侧输入张量\n",
    "# p(float, optional) - 要计算的范数\n",
    "# out(Tensor, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]), tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(4,4)\n",
    "b = torch.ones(4,4)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(a, b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cumsum(input, dim, out=None) -> Tensor: 返回输入沿指定维度的累积和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4.],\n",
       "         [1., 2., 3., 4.],\n",
       "         [1., 2., 3., 4.],\n",
       "         [1., 2., 3., 4.]]), tensor([[1., 1., 1., 1.],\n",
       "         [2., 2., 2., 2.],\n",
       "         [3., 3., 3., 3.],\n",
       "         [4., 4., 4., 4.]]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(b, dim = 1) , torch.cumsum(b, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.norm(input, p=2) -> float:\n",
    "# 返回输入张量input的p范数。\n",
    "\n",
    "# input(Tensor) - 输入张量\n",
    "# p(float, optional) - 范数计算中的幂指数值\n",
    "\n",
    "# torch.norm(input, p, dim, out=None) -> Tensor:\n",
    "# 返回输入张量给定维度dim上每行的p范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(b, 2, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.prod(input) -> float:\n",
    "# 返回输入张量input所有元素的积\n",
    "\n",
    "# torch.prod(input, dim, out=None) -> Tensor:\n",
    "# 返回输入张量给定维度上每行的积。\n",
    "\n",
    "# torch.std(input) -> float:\n",
    "# 返回输入张量input所有元素的标准差\n",
    "\n",
    "# torch.std(input, dim, out=None):\n",
    "# 返回输入张量给定维度上每行的标准差。\n",
    "\n",
    "# torch.sum(input) -> float:\n",
    "# 返回输入张量input所有元素的各\n",
    "\n",
    "# torch.sum(input, dim, out=None) -> Tensor:\n",
    "# 返回输入疑是给定维度上每行的和\n",
    "\n",
    "# torch.var(input) -> float:\n",
    "# 返回输入张量所有元素的方差\n",
    "\n",
    "# torch.var(input, dim, out=None) -> Tensor:\n",
    "# 返回输入张量给定维度上每行的方差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 7])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = torch.arange(1, 10, step=3, out=None)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.prod(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [2., 2., 2.],\n",
       "        [3., 3., 3.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_2 = torch.cumsum(torch.ones(3,3), dim = 0)\n",
    "tr_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(216.)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.prod(tr_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3**3 * 2**3 * 1**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.3 比较操作Comparison Ops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.eq(input, other, out=None) -> Tensor:\n",
    "# 比较元素相等性，第二个参数可为一个数，或与第一个参数同类型形状的张量\n",
    "\n",
    "# input(Tensor) - 待比较张量\n",
    "# other(Tensor or float) - 比较张量或数\n",
    "# out(Tensor, optional) - 输出张量，须为ByteTensor类型或与input同类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False],\n",
       "        [False,  True]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#若两个张量有相同的形状和元素值，则返回True， 否则False。\n",
    "torch.equal(torch.tensor([[1, 2], [3, 4]]),torch.tensor([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.ge(input, other, out=None) -> Tensor:\n",
    "# 逐元素比较input和other，即是否input >= other\n",
    "# 第二个参数可以为一个数或与第一个参数相同形状和类型的张量。\n",
    "\n",
    "# input(Tensor) - 待对比的张量\n",
    "# other(Tensor or float) - 对比的张量或float值\n",
    "# out(Tensor, optional) - 输出张量，必须为ByteTensor或与第一个参数相同类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True],\n",
       "        [False,  True]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.gt(input, other, out=None) -> Tensor:\n",
    "# 逐元素比较input和other，是否input > other。若两个张量有相同的形状和元素值，则返回True，否则False。第二个参数\n",
    "# 可以为一个数或与第一个参数相同形状和类型的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True],\n",
       "        [False, False]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.kthvalue(input, k, dim=None, out=None) -> (Tensor, LongTensor):\n",
    "# 取输入张量input指定维度上第k个最小值，若不指定dim，则默认为input的最后一维。返回一个元组，其中indices是原始输入张量input中沿dim维的第k个最小值下标。\n",
    "\n",
    "# input(Tensor) - 输入张量\n",
    "# k(int) - 第k个最小值\n",
    "# dim(int, optional)` - 沿着此维度进行排序\n",
    "# out(tuple, optional) - 输出元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5475, 0.0857],\n",
       "        [0.6618, 0.3216],\n",
       "        [0.5023, 0.3182],\n",
       "        [0.2254, 0.8151]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(\n",
       "values=tensor([0.0857, 0.3216, 0.3182, 0.2254]),\n",
       "indices=tensor([1, 1, 1, 0]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.kthvalue(x, 1, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.le(input, other, out=None) -> Tensor:\n",
    "# 逐元素比较input和other，即是否input <= other，第二个参数可以为一个数或与第一个参数相同形状和类型的张量。\n",
    "\n",
    "# torch.lt(input, other, out=None) -> Tensor:\n",
    "# 逐元素比较input和other，即是否input < other\n",
    "\n",
    "# torch.ne(input, other, out=Tensor) -> Tensor:\n",
    "# 逐元素比较input和other， 即是否input != other。第二个参数可以为一个数或与第一个参数相同形状和类型的张量。\n",
    "# 返回值：一个torch.ByteTensor张量，包含了每个位置的比较结果(如果tensor != other 为True，返回1)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True],\n",
       "        [ True, False]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.le(torch.tensor([[1, 2], [3, 4]]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.max(input, dim, max=None, max_indice=None) -> (Tensor, LongTensor):\\n返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。\\n\\ninput(Tensor) - 输入张量\\ndim(int) - 指定的维度\\nmax(Tensor, optional) - 结果张量，包含给定维度上的最大值\\nmax_indices(LongTensor, optional) - 包含给定维度上每个最大值的位置索引。\\n\\ntorch.min(input, dim, min=None, min_indices=None) -> (Tensor, LongTensor):\\n返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引。\\n\\ntorch.min(input, other, out=None) -> Tensor:\\ninput中逐元素与other相应位置的元素对比，返回最小值到输出张量。\\n两张量形状不需匹配，但元素数须相同。\\n'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.max(input, dim, max=None, max_indice=None) -> (Tensor, LongTensor):\n",
    "# 返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。\n",
    "\n",
    "# input(Tensor) - 输入张量\n",
    "# dim(int) - 指定的维度\n",
    "# max(Tensor, optional) - 结果张量，包含给定维度上的最大值\n",
    "# max_indices(LongTensor, optional) - 包含给定维度上每个最大值的位置索引。\n",
    "\n",
    "# torch.min(input, dim, min=None, min_indices=None) -> (Tensor, LongTensor):\n",
    "# 返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引。\n",
    "\n",
    "# torch.min(input, other, out=None) -> Tensor:\n",
    "# input中逐元素与other相应位置的元素对比，返回最小值到输出张量。\n",
    "# 两张量形状不需匹配，但元素数须相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8817, -0.4408, -1.7436,  0.4526],\n",
       "        [-0.7211, -0.4477,  0.0557, -2.0263],\n",
       "        [-2.3626, -1.2202,  1.4129,  1.2837],\n",
       "        [-0.8807,  0.5810, -0.4354, -0.0882]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=torch.randn(4, 4)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.4526, 0.0557, 1.4129, 0.5810]),\n",
       "indices=tensor([3, 2, 2, 1]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor):\n",
    "# 对输入张量input沿着指定维度按升序排序，如果不给定dim，默认为输入的最后一维。如果指定参数descending为True，则按降序排序。\n",
    "# 返回两项：重排后的张量，和重排后元素在原张量的索引\n",
    "\n",
    "# input(Tensor) - 输入张量\n",
    "# dim(int, optional) - 沿此维排序，默认为最后一维\n",
    "# descending(bool, optional) - 布尔值，默认升序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[-2.3626, -1.2202, -1.7436, -2.0263],\n",
       "        [-0.8817, -0.4477, -0.4354, -0.0882],\n",
       "        [-0.8807, -0.4408,  0.0557,  0.4526],\n",
       "        [-0.7211,  0.5810,  1.4129,  1.2837]]),\n",
       "indices=tensor([[2, 2, 0, 1],\n",
       "        [0, 1, 3, 3],\n",
       "        [3, 0, 1, 0],\n",
       "        [1, 3, 2, 2]]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sort(k,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor):\n",
    "# 沿给定dim维度返回输入张量input中k个最大值，不指定dim，\n",
    "# 则默认为最后一维，如果largest为False，则返回最小的k个值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[-0.7211,  0.5810,  1.4129,  1.2837],\n",
       "        [-0.8807, -0.4408,  0.0557,  0.4526]]),\n",
       "indices=tensor([[1, 3, 2, 2],\n",
       "        [3, 0, 1, 0]]))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(k, 2,dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4 其它操作 Other Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cross(input, other, dim=-1, out=None) -> Tensor:\n",
    "# 返回沿着维度dim上，两个张量input和other的叉积。input和other必须有相同的形状，\n",
    "# input and other must have the same size, and the size of their dim dimension should be 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]]), tensor([[0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.]]), tensor([[0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.]]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tora = torch.FloatTensor([[1,0,0],[1,0,0],[1,0,0]])\n",
    "torb = torch.FloatTensor([[0,1,0],[0,1,0],[0,1,0]])\n",
    "torc = torch.FloatTensor([[0,1,0],[0,1,0],[0,1,0]])\n",
    "tora,torb,torc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tora.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]]), tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cross(tora, torb,dim=1), torch.cross(tora, torb,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]), tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cross(torc, torb, dim=1), torch.cross(torc, torb, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dimension 0 does not have size 3",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-03eb511e5611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: dimension 0 does not have size 3"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "a = torch.ones(1, 3)*2\n",
    "b = torch.ones(1, 3)*3\n",
    "torch.cross(a, b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cross(a, b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]]), tensor([[3., 3., 3.],\n",
       "         [3., 3., 3.],\n",
       "         [3., 3., 3.],\n",
       "         [3., 3., 3.]]))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(4, 3)*2\n",
    "b = torch.ones(4, 3)*3\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cross(a, b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dimension 0 does not have size 3",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-f89f00030b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: dimension 0 does not have size 3"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "torch.cross(a, b, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary:\n",
    "-1 纬度必须是3，至少你要计算的那个纬度必须是3  \n",
    "-2 矩阵的外积计算是在你指定的纬度方向上来计算对应的两个向量的外积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.diag(input, diagonal=0, out=None) -> Tensor:\n",
    "# 如果输入是一个向量，则返回一个以input为对角线元素的2D方阵\n",
    "# 如果输入是一个矩阵，则返回一个包含input为对角元素的1D张量\n",
    "# 参数diagonal指定对角线：\n",
    "\n",
    "# diagonal = 0, 主对角线\n",
    "# diagonal > 0, 主对角线之上\n",
    "# diagonal < 0, 主对角线之下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7895, 0.4159, 1.5771, 0.4652])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randn(4)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7895, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.4159, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.5771, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.4652]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(b, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.7895, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.4159, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 1.5771, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.4652],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6075, -1.2946,  1.2443],\n",
       "        [-0.4918, -0.9993,  0.3192],\n",
       "        [-0.5449, -1.6094,  0.1476],\n",
       "        [-1.3800, -1.0467, -1.1337]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6075, -0.9993,  0.1476])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2946,  0.3192])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(a, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.5 BLAS and LAPACK Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算两个张量的点乘，两个张量都为1-D向量\n",
    "torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4595, -0.8103,  0.9959],\n",
       "        [ 0.6419, -1.6021, -0.2116],\n",
       "        [-0.0101,  0.0822,  0.1004]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9383,  1.0678, 11.5578],\n",
       "        [-0.4075, -0.2361,  3.5452],\n",
       "        [ 0.2397,  0.3006,  8.2171]])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对方阵input求逆\n",
    "ai = torch.inverse(a)\n",
    "ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  3.7592e-07, -1.8304e-08],\n",
       "        [ 1.0544e-08,  1.0000e+00, -3.4446e-10],\n",
       "        [ 5.7823e-09,  3.1052e-09,  1.0000e+00]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对矩阵mat1和mat2进行相乘\n",
    "torch.mm(ai, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]]), tensor([1., 1., 1.]))"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对矩阵mat和向量vec进行相乘。\n",
    "v = torch.ones(3)\n",
    "m=torch.ones(3,3)*2\n",
    "m,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3]), torch.Size([3]))"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6.])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(m, v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "matrix and vector expected, got 1D, 2D at ../aten/src/TH/generic/THTensorMath.cpp:343",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-318-09ec00731946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: matrix and vector expected, got 1D, 2D at ../aten/src/TH/generic/THTensorMath.cpp:343"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "torch.mv(v, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = torch.ones(3,1)\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]]), tensor([[1.],\n",
       "         [1.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.mv(m, v2)   #error: 因为此时是两个矩阵相乘，不可以用.mv 应该用.mm\n",
    "m,v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.],\n",
       "        [6.],\n",
       "        [6.]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(m, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *  逐元素相乘:  矩阵与向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [3.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2[1]=3\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]]), tensor([[1.],\n",
       "         [3.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.],\n",
       "        [6., 6., 6.],\n",
       "        [2., 2., 2.]])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m*v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]]), tensor([1., 1., 1.]))"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 2.5000])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[2]=2.5\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 5.],\n",
       "        [2., 2., 5.],\n",
       "        [2., 2., 5.]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te=m*v\n",
    "te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *  逐元素相乘:  矩阵与矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.2000, 5.2000],\n",
       "         [5.2000, 5.2000]]), tensor([[2., 2.],\n",
       "         [2., 2.]]))"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2, 2)*5.2\n",
    "b = torch.ones(2, 2)*2\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.4000, 10.4000],\n",
       "        [10.4000, 10.4000]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.eig(a, eigenvectors=False, out=None) -> (Tensor, Tensor):\n",
    "# 计算方阵a的特征值和特征向量。\n",
    "\n",
    "# a(Tensor) - 方阵\n",
    "# eigenvectors(bool) - 如果为True，同时计算特征值和特征微量，否则只计算特征值\n",
    "# 返回值：\n",
    "# e(Tensor) - a的右特征向量\n",
    "# v(Tensor) - 如果eigenvectors为True，则为包含特征向量的张量，否则为空。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 0.],\n",
       "        [1., 2., 1.],\n",
       "        [0., 1., 2.]])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toreigen = torch.FloatTensor([[2,1,0],[1,2,1],[0,1,2]])\n",
    "toreigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[0.5858, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [3.4142, 0.0000]]),\n",
       "eigenvectors=tensor([[ 5.0000e-01,  7.0711e-01,  5.0000e-01],\n",
       "        [-7.0711e-01,  1.2491e-07,  7.0711e-01],\n",
       "        [ 5.0000e-01, -7.0711e-01,  5.0000e-01]]))"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eig(toreigen, eigenvectors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenV1 = 0.58  eigenV2 = 2  eigenV3 = 3.41\n",
    "# 下面的结果经过了正则化\n",
    "# eigenVec1= [ 5.0000e-01,  7.0711e-01,  5.0000e-01]\n",
    "# eigenVec2= [-7.0711e-01,  1.2491e-07,  7.0711e-01]\n",
    "# eigenVec3= [ 5.0000e-01,  -7.0711e-01,  5.0000e-01]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
