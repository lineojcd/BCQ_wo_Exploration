{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://www.jianshu.com/p/d678c5e44a6b\n",
    "torch常用基础函数    \n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F     # 激励函数都在这\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "# python 的可视化模块, 我有教程 (https://morvanzhou.github.io/tutorials/data-manipulation/plt/)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.张量Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3023, 0.1208, 0.1272],\n",
       "        [0.1766, 0.5085, 0.8517],\n",
       "        [0.5513, 0.9506, 0.6067],\n",
       "        [0.8191, 0.4089, 0.0038],\n",
       "        [0.6152, 0.8712, 0.9641]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果obj是一个pytorch张量，则返回True\n",
    "torch.is_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果obj是一个pytorch storage对象，则返回True\n",
    "torch.is_storage(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回input张量中的元素个数\n",
    "torch.numel(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 创建操作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.eye(n, m=None, out=None):\n",
    "返回一个2维张量，对角线为1,其它位置为0\n",
    "\n",
    "n (int) -行数\n",
    "m (int, optional)列数，如果为None,则默认为n\n",
    "out (Tensor, optional)\n",
    "\"\"\"\n",
    "torch.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nda = np.array([[1, 2], [3, 4]])\n",
    "print(type(nda))\n",
    "nda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nda_tr = torch.from_numpy(nda)\n",
    "nda_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n将numpy.ndarray转换为Tensor，返回的张量tensor和numpy的ndarray共享同一内存空间，\\n修改一个会导致另一个也被修改，返回的张量不能改变大小\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "将numpy.ndarray转换为Tensor，返回的张量tensor和numpy的ndarray共享同一内存空间，\n",
    "修改一个会导致另一个也被修改，返回的张量不能改变大小\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 9],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nda[0,1]=9\n",
    "nda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 9],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nda_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.linspace(start, end, steps=100, out=None):\\n返回一个1维张量，包含在start和end上均匀间隔的steps个点\\n\\nstart (float) -序列起点\\nend (float) - 序列终点\\nsteps (int) - 在start与end间生成的样本数\\nout (Tensor, optional) - 结果张量\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.linspace(start, end, steps=100, out=None):\n",
    "返回一个1维张量，包含在start和end上均匀间隔的steps个点\n",
    "\n",
    "start (float) -序列起点\n",
    "end (float) - 序列终点\n",
    "steps (int) - 在start与end间生成的样本数\n",
    "out (Tensor, optional) - 结果张量\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.,  12.,  23.,  34.,  45.,  56.,  67.,  78.,  89., 100.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1, 100, steps=10,out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.logspace(start, end, steps=100, out=None):\\n返回一个1维张量，包含在区间10exp(start)和10exp(end)上以对数刻度均匀间隔的 steps个点。\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.logspace(start, end, steps=100, out=None):\n",
    "返回一个1维张量，包含在区间10exp(start)和10exp(end)上以对数刻度均匀间隔的 steps个点。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WWY start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0000e+02, 1.0000e+04, 1.0000e+06, 1.0000e+08, 1.0000e+10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logspace(0, 10, steps=6,out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.ones(*sizes, out=None):\\n返回一个全为1的张量，形状由可变参数sizes定义\\n\\nsizes (int...) - 整数序列，定义了输出形状\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.ones(*sizes, out=None):\n",
    "返回一个全为1的张量，形状由可变参数sizes定义\n",
    "\n",
    "sizes (int...) - 整数序列，定义了输出形状\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,4, out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.rand(*sizes, out=None):\\n返回一个张量，包含了从区间(0, 1)的均匀分布中抽取的一组随机数，形状由可变参数sizes定义。\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.rand(*sizes, out=None):\n",
    "返回一个张量，包含了从区间(0, 1)的均匀分布中抽取的一组随机数，形状由可变参数sizes定义。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9248, 0.0830, 0.6693, 0.6845, 0.8389],\n",
       "        [0.5217, 0.4494, 0.5006, 0.4137, 0.5997],\n",
       "        [0.4796, 0.6178, 0.9578, 0.1058, 0.3239]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.randn(*sizes, out=None):\\n返回一个张量，包含了从标准正态分布(mean=0, std=1)中抽取一组随机数，形状由可变参数sizes定义。\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.randn(*sizes, out=None):\n",
    "返回一个张量，包含了从标准正态分布(mean=0, std=1)中抽取一组随机数，形状由可变参数sizes定义。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2397,  0.6845,  0.7244, -1.8536],\n",
       "        [-0.0914, -0.2976,  1.0421, -0.9732],\n",
       "        [ 2.1047,  0.0220,  1.3665, -0.6199]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.randperm(n, out=None):\\n给定参数n，返回一个从0到n-1的随机整数排列\\n\\nn (int) - 上边界(不包含）\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.randperm(n, out=None):\n",
    "给定参数n，返回一个从0到n-1的随机整数排列\n",
    "\n",
    "n (int) - 上边界(不包含）\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 2, 1, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.arange(start, end, step=1, out=None):\\n返回一个1维张量，长度为floor((end-start)/step)，以step`为步长的一组序列值。\\n\\nstart (float) - 起点\\nend (float) - 终点(不包含）\\nstep (float) - 相邻点的间隔大小\\nout (Tensor, optional)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.arange(start, end, step=1, out=None):\n",
    "返回一个1维张量，长度为floor((end-start)/step)，以step`为步长的一组序列值。\n",
    "\n",
    "start (float) - 起点\n",
    "end (float) - 终点(不包含）\n",
    "step (float) - 相邻点的间隔大小\n",
    "out (Tensor, optional)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 7])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10, step=3, out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/spinningup/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  4.,  7., 10.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.range(1, 10, step=3, out=None)\n",
    "# 推荐使用torch.arange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.zeros(*sizes, out=None):\\n返回一个全为标量0的张量，形状由可变参数sizes定义\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.zeros(*sizes, out=None):\n",
    "返回一个全为标量0的张量，形状由可变参数sizes定义\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 索引，切片，连接，换位(Index, Slicing, Joining, Mutating)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.cat(inputs, dimension=0):\\n在给定维度上对输入的张量序列seq进行连接操作。\\n\\ninputs (sequence of Tensors)\\ndimension (int optional) - 沿着此维连接张量序列\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.cat(inputs, dimension=0):\n",
    "在给定维度上对输入的张量序列seq进行连接操作。\n",
    "\n",
    "inputs (sequence of Tensors)\n",
    "dimension (int optional) - 沿着此维连接张量序列\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3023, 0.1208, 0.1272],\n",
       "        [0.1766, 0.5085, 0.8517],\n",
       "        [0.5513, 0.9506, 0.6067],\n",
       "        [0.8191, 0.4089, 0.0038],\n",
       "        [0.6152, 0.8712, 0.9641]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3023, 0.1208, 0.1272, 0.3023, 0.1208, 0.1272, 0.3023, 0.1208, 0.1272],\n",
       "        [0.1766, 0.5085, 0.8517, 0.1766, 0.5085, 0.8517, 0.1766, 0.5085, 0.8517],\n",
       "        [0.5513, 0.9506, 0.6067, 0.5513, 0.9506, 0.6067, 0.5513, 0.9506, 0.6067],\n",
       "        [0.8191, 0.4089, 0.0038, 0.8191, 0.4089, 0.0038, 0.8191, 0.4089, 0.0038],\n",
       "        [0.6152, 0.8712, 0.9641, 0.6152, 0.8712, 0.9641, 0.6152, 0.8712, 0.9641]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.chunk(tensor, chunks, dim=0):\\n在给定维度上将输入张量进行分块\\n\\ntensors(Tensors) - 待分场的输入张量\\nchunks (int) - 分块的个数\\ndim (int) - 沿着此维度\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.chunk(tensor, chunks, dim=0):\n",
    "在给定维度上将输入张量进行分块\n",
    "\n",
    "tensors(Tensors) - 待分场的输入张量\n",
    "chunks (int) - 分块的个数\n",
    "dim (int) - 沿着此维度\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3023, 0.1208],\n",
       "         [0.1766, 0.5085],\n",
       "         [0.5513, 0.9506],\n",
       "         [0.8191, 0.4089],\n",
       "         [0.6152, 0.8712]]),\n",
       " tensor([[0.1272],\n",
       "         [0.8517],\n",
       "         [0.6067],\n",
       "         [0.0038],\n",
       "         [0.9641]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(x, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.gather(input, dim, index, out=None):\\n沿给定轴dim,将输入索引张量index指定位置的值进行聚合。\\n\\ninput(Tensor) - 源张量\\ndim(int) - 索引的轴\\nindex(LongTensor) - 聚合元素的下标\\nout - 目标张量\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.gather(input, dim, index, out=None):\n",
    "沿给定轴dim,将输入索引张量index指定位置的值进行聚合。\n",
    "\n",
    "input(Tensor) - 源张量\n",
    "dim(int) - 索引的轴\n",
    "index(LongTensor) - 聚合元素的下标\n",
    "out - 目标张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3023, 0.1208],\n",
       "        [0.5085, 0.5085],\n",
       "        [0.6067, 0.6067],\n",
       "        [0.8191, 0.8191],\n",
       "        [0.9641, 0.8712]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(x, 1, torch.LongTensor([[0,1],[1,1],[2,2],[0,0],[2,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.index_select(input, dim, index, out=None):\\n沿指定维度对输入进行切片，取index中指定的相应项，然后返回一个新的张量，返回的张量与原始张量有相同的维度(在指定轴上)，返回的张量与原始张量不共享内存空间\\n\\ninput(Tensor) - 输入张量\\ndim(int) - 索引的轴\\nindex(LongTensor) - 包含索引下标的一维张量\\nout - 目标张量\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.index_select(input, dim, index, out=None):\n",
    "沿指定维度对输入进行切片，取index中指定的相应项，然后返回一个新的张量，返回的张量与原始张量有相同的维度(在指定轴上)，返回的张量与原始张量不共享内存空间\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "dim(int) - 索引的轴\n",
    "index(LongTensor) - 包含索引下标的一维张量\n",
    "out - 目标张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3023, 0.1208, 0.1272],\n",
       "        [0.5513, 0.9506, 0.6067]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(x, 0, torch.tensor([0,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.masked_select(input, mask, out=None):\\n根据掩码张量mask中的二元值，取输入张量中的指定项，将取值返回到一个新的1D张量。\\n张量mask须跟input张量有相同的元素数目，但形状或维度不需要相同。返回的张量不与原始张量共享内存空间\\n\\ninput(Tensor) - 输入张量\\nmask(ByteTensor) - 掩码张量，包含了二元索引值\\nout - 目标张量\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.masked_select(input, mask, out=None):\n",
    "根据掩码张量mask中的二元值，取输入张量中的指定项，将取值返回到一个新的1D张量。\n",
    "张量mask须跟input张量有相同的元素数目，但形状或维度不需要相同。返回的张量不与原始张量共享内存空间\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "mask(ByteTensor) - 掩码张量，包含了二元索引值\n",
    "out - 目标张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/distiller/project/conda/conda-bld/pytorch_1579022036889/work/aten/src/ATen/native/LegacyDefinitions.cpp:70: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3023, 0.1208, 0.1766, 0.5085, 0.5513, 0.9506, 0.0038, 0.6152, 0.9641])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ByteTensor([[1,1,0],[1,1,0],[1,1,0], [0,0,1], [1,0,1]])\n",
    "torch.masked_select(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.nonzero(input, out=None):\\n返回一个包含输入input中非零元素索引的张量，输出张量中的每行包含输入中非零元素的索引\\n若输入input有n维，则输出的索引张量output形状为z * n, 这里z是输入张量input中所有非零元素的个数\\n\\ninput(Tensor) - 输入张量\\nout - 包含索引值的结果张量\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.nonzero(input, out=None):\n",
    "返回一个包含输入input中非零元素索引的张量，输出张量中的每行包含输入中非零元素的索引\n",
    "若输入input有n维，则输出的索引张量output形状为z * n, 这里z是输入张量input中所有非零元素的个数\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "out - 包含索引值的结果张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 1],\n",
       "        [0, 2],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [2, 0],\n",
       "        [2, 1],\n",
       "        [2, 2],\n",
       "        [3, 0],\n",
       "        [3, 1],\n",
       "        [3, 2],\n",
       "        [4, 0],\n",
       "        [4, 1],\n",
       "        [4, 2]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.split(tensor, split_size, dim=0):\\n将输入张量分割成相等形状的chunks(如果可分)。如果沿指定维的张量形状大小不能被split_size整分，则最后一个分块会小于其它分块。\\n\\ntensor(Tensor) - 待分割张量\\nsplit_size(int) - 单个分块的形状大小\\ndim(int) - 沿着此维进行分割\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.split(tensor, split_size, dim=0):\n",
    "将输入张量分割成相等形状的chunks(如果可分)。如果沿指定维的张量形状大小不能被split_size整分，则最后一个分块会小于其它分块。\n",
    "\n",
    "tensor(Tensor) - 待分割张量\n",
    "split_size(int) - 单个分块的形状大小\n",
    "dim(int) - 沿着此维进行分割\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3023, 0.1208, 0.1272],\n",
       "         [0.1766, 0.5085, 0.8517],\n",
       "         [0.5513, 0.9506, 0.6067]]),\n",
       " tensor([[0.8191, 0.4089, 0.0038],\n",
       "         [0.6152, 0.8712, 0.9641]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(x, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.squeeze(input, dim=None, out=None):\\n将输入张量形状中的1去除并返回，如果输入是形如(A * 1 * B * 1 * C * 1 *D)，那么输出形状就为：(A * B * C * D)。\\n当给定dim时，则只在给定维度上进行挤压，如输入形状为(A * 1 * B)，squeeze(input, 0)，将会保持张量不变，只有用squeeze(input, 1)，形状会变成(A *B)。\\n输入张量与返回张量共享内存\\n\\ninput(Tensor) - 输入张量\\ndim(int, optional) - 如果给定，则只在给定维度挤压\\nout(Tensor, optional) - 输出张量\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.squeeze(input, dim=None, out=None):\n",
    "将输入张量形状中的1去除并返回，如果输入是形如(A * 1 * B * 1 * C * 1 *D)，那么输出形状就为：(A * B * C * D)。\n",
    "当给定dim时，则只在给定维度上进行挤压，如输入形状为(A * 1 * B)，squeeze(input, 0)，将会保持张量不变，只有用squeeze(input, 1)，形状会变成(A *B)。\n",
    "输入张量与返回张量共享内存\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "dim(int, optional) - 如果给定，则只在给定维度挤压\n",
    "out(Tensor, optional) - 输出张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(2, 1, 2, 1, 2)\n",
    "torch.squeeze(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.stack(sequence, dim=0):\\n沿着一个新维度对输入张量进行连接，序列中所有张量都应该为相同的形状。\\n\\nsequence(Sequence) - 待连接的张量序列\\ndim(int) - 插入的维度\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.stack(sequence, dim=0):\n",
    "沿着一个新维度对输入张量进行连接，序列中所有张量都应该为相同的形状。\n",
    "\n",
    "sequence(Sequence) - 待连接的张量序列\n",
    "dim(int) - 插入的维度\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3023, 0.1208, 0.1272],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.1766, 0.5085, 0.8517],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.5513, 0.9506, 0.6067],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.8191, 0.4089, 0.0038],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.6152, 0.8712, 0.9641],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(5,3)\n",
    "z = torch.ones(5,3)\n",
    "torch.stack([x, y, z], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.t(input, out=None):\\n输入一个矩阵(2维张量)，并转置0,1维，可以被视为transpose(input, 0, 1)的简写函数\\n\\ninput(Tensor) - 输入张量\\nout(Tensor, optional) - 结果张量\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.t(input, out=None):\n",
    "输入一个矩阵(2维张量)，并转置0,1维，可以被视为transpose(input, 0, 1)的简写函数\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "out(Tensor, optional) - 结果张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3023, 0.1766, 0.5513, 0.8191, 0.6152],\n",
       "        [0.1208, 0.5085, 0.9506, 0.4089, 0.8712],\n",
       "        [0.1272, 0.8517, 0.6067, 0.0038, 0.9641]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.transpose(input, dim0, dim1, out=None):\\n返回输入矩阵input的转置，交换维度dim0和dim1。输入张量与输出张量共享内存。\\n\\ninput(Tensor) - 输入张量\\ndim0(int) - 转置的第一维\\ndim1(int) - 转置的第二维\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.transpose(input, dim0, dim1, out=None):\n",
    "返回输入矩阵input的转置，交换维度dim0和dim1。输入张量与输出张量共享内存。\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "dim0(int) - 转置的第一维\n",
    "dim1(int) - 转置的第二维\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3023, 0.1766, 0.5513, 0.8191, 0.6152],\n",
       "        [0.1208, 0.5085, 0.9506, 0.4089, 0.8712],\n",
       "        [0.1272, 0.8517, 0.6067, 0.0038, 0.9641]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(x, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.unbind(tensor, dim=0)[source]:\\n移除指定维度后，返回一个元组，包含了沿着指定维切片后的各个切片\\n\\ntensor(Tensor) - 输入张量\\ndim(int) - 删除的维度\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.unbind(tensor, dim=0)[source]:\n",
    "移除指定维度后，返回一个元组，包含了沿着指定维切片后的各个切片\n",
    "\n",
    "tensor(Tensor) - 输入张量\n",
    "dim(int) - 删除的维度\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3023, 0.1766, 0.5513, 0.8191, 0.6152]),\n",
       " tensor([0.1208, 0.5085, 0.9506, 0.4089, 0.8712]),\n",
       " tensor([0.1272, 0.8517, 0.6067, 0.0038, 0.9641]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.unsqueeze(input, dim, out=None):\\n返回一个新的张量，对输入的指定位置插入维度1，返回张量与输入张量共享内存，若dim为负，则将被转化为dim+input.dim()+1\\n\\ntensor(Tensor) - 输入张量\\ndim(int) - 插入维度的索引\\nout(Tensor, optional) - 结果张量\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.unsqueeze(input, dim, out=None):\n",
    "返回一个新的张量，对输入的指定位置插入维度1，返回张量与输入张量共享内存，若dim为负，则将被转化为dim+input.dim()+1\n",
    "\n",
    "tensor(Tensor) - 输入张量\n",
    "dim(int) - 插入维度的索引\n",
    "out(Tensor, optional) - 结果张量\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3023, 0.1208, 0.1272]],\n",
       "\n",
       "        [[0.1766, 0.5085, 0.8517]],\n",
       "\n",
       "        [[0.5513, 0.9506, 0.6067]],\n",
       "\n",
       "        [[0.8191, 0.4089, 0.0038]],\n",
       "\n",
       "        [[0.6152, 0.8712, 0.9641]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 随机抽样Random sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.manual_seed(seed)\\n设定生成随机数的种子，并返回一个torch._C.Generator对象\\n参数: seed (int or long) – 种子\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.manual_seed(seed)\n",
    "设定生成随机数的种子，并返回一个torch._C.Generator对象\n",
    "参数: seed (int or long) – 种子\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12055c270>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#返回生成随机数的原始种子值（python long）\n",
    "\n",
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.get_rng_state()[source]\\n返回随机生成器状态(ByteTensor)\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.get_rng_state()[source]\n",
    "返回随机生成器状态(ByteTensor)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 0,  ..., 0, 0, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng_state = torch.get_rng_state()\n",
    "rng_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.set_rng_state(new_state)[source]:\\n设定随机生成器状态参数：new_state(torch.ByteTensor) - 期望的状态\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.set_rng_state(new_state)[source]:\n",
    "设定随机生成器状态参数：new_state(torch.ByteTensor) - 期望的状态\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_rng_state(rng_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12055c270>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.default_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.bernoulli(input, out=None):\\n从伯努利分布中抽取二元随机数(0或者1），输入中所有值必须在[0, 1]区间，输出张量的第i个元素值，将以输入张量的第i个概率值等于1。\\n返回值将会是与输入相同大小的张量，每个值为0或1\\n\\ninput(Tensor) - 输入为伯努利分布的概率值(probability of drawing \"1\")\\nout(Tensor, optional)\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.bernoulli(input, out=None):\n",
    "从伯努利分布中抽取二元随机数(0或者1），输入中所有值必须在[0, 1]区间，输出张量的第i个元素值，将以输入张量的第i个概率值等于1。\n",
    "返回值将会是与输入相同大小的张量，每个值为0或1\n",
    "\n",
    "input(Tensor) - 输入为伯努利分布的概率值(probability of drawing \"1\")\n",
    "out(Tensor, optional)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(3,5)\n",
    "torch.bernoulli(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.multinomial(input, num_samples, replacement=False, out=None):\\n返回一个张量，每行包含从input相应行中定义的多项式分布中抽取的num_samples个样本。\\ninput每行的值不需要总和为1,但必须非负且总和不能为0.\\n\\n当抽取样本时，依次从左到右排列(第一个样本对应第一列)。\\n\\n如果输入input是一个向量，输出out也是一个相同长度num_samples的向量。如果输入input是有 m行的矩阵，输出out是形如m×n的矩阵。\\n\\n如果参数replacement 为 True, 则样本抽取可以重复。否则，一个样本在每行不能被重复抽取。\\n\\n参数num_samples必须小于input长度(即，input的列数，如果是input是一个矩阵)。\\n\\ninput(Tensor) - 包含概率值的张量\\nnum_samples(int) - 抽取的样本数\\nreplacement(bool, optional) - 布尔值，决定是否能重复抽取\\nout(Tensor, optional)\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.multinomial(input, num_samples, replacement=False, out=None):\n",
    "返回一个张量，每行包含从input相应行中定义的多项式分布中抽取的num_samples个样本。\n",
    "input每行的值不需要总和为1,但必须非负且总和不能为0.\n",
    "\n",
    "当抽取样本时，依次从左到右排列(第一个样本对应第一列)。\n",
    "\n",
    "如果输入input是一个向量，输出out也是一个相同长度num_samples的向量。如果输入input是有 m行的矩阵，输出out是形如m×n的矩阵。\n",
    "\n",
    "如果参数replacement 为 True, 则样本抽取可以重复。否则，一个样本在每行不能被重复抽取。\n",
    "\n",
    "参数num_samples必须小于input长度(即，input的列数，如果是input是一个矩阵)。\n",
    "\n",
    "input(Tensor) - 包含概率值的张量\n",
    "num_samples(int) - 抽取的样本数\n",
    "replacement(bool, optional) - 布尔值，决定是否能重复抽取\n",
    "out(Tensor, optional)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [2, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 2]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.normal(means, std, out=None):\\n返回一个张量，包含从给定means, std的离散正态分布中抽取随机数，均值和标准差的形状不须匹配，但每个张量的元素个数须相同\\n\\nmeans(Tensor) - 均值\\nstd(Tensor) - 标准差\\nout(Tensor, optional)\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.normal(means, std, out=None):\n",
    "返回一个张量，包含从给定means, std的离散正态分布中抽取随机数，均值和标准差的形状不须匹配，但每个张量的元素个数须相同\n",
    "\n",
    "means(Tensor) - 均值\n",
    "std(Tensor) - 标准差\n",
    "out(Tensor, optional)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2601,  2.0472,  3.7470,  4.3432,  4.8692,  6.0816,  7.6030,  8.1062,\n",
       "         8.8146, 10.0819])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.6760, 1.9552, 2.4833, 4.4098]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(2, 3, size=(1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 序列化 Serialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.save(obj, f, pickle_module, pickle_protocol=2):\\n保存一个对象到一个硬盘文件上\\n\\nobj - 保存对象\\nf - 类文件对象\\npickle_module - 用于pickling元数据和对象的模块\\npickle_protocol - 指定pickle protocal可以覆盖默认参数\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.save(obj, f, pickle_module, pickle_protocol=2):\n",
    "保存一个对象到一个硬盘文件上\n",
    "\n",
    "obj - 保存对象\n",
    "f - 类文件对象\n",
    "pickle_module - 用于pickling元数据和对象的模块\n",
    "pickle_protocol - 指定pickle protocal可以覆盖默认参数\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x, 'tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.load(f, map_location=None, pickle_module=):\\n从磁盘文件中读取一个通过torch.save()保存的对象，可通过参数map_location动态地进行内存重映射\\n\\nf - 类文件对象\\nmap_location - 一个函数或字典规定如何remap存储位置\\npickle_module - 用于unpickling元数据和对象的模块\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.load(f, map_location=None, pickle_module=):\n",
    "从磁盘文件中读取一个通过torch.save()保存的对象，可通过参数map_location动态地进行内存重映射\n",
    "\n",
    "f - 类文件对象\n",
    "map_location - 一个函数或字典规定如何remap存储位置\n",
    "pickle_module - 用于unpickling元数据和对象的模块\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3023, 0.1208, 0.1272],\n",
       "        [0.1766, 0.5085, 0.8517],\n",
       "        [0.5513, 0.9506, 0.6067],\n",
       "        [0.8191, 0.4089, 0.0038],\n",
       "        [0.6152, 0.8712, 0.9641]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('tensor.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 并行化 Parallelism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获得用于并行化CPU操作的OpenMP线程数\n",
    "\n",
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设定用于并行化CPU操作的OpenMP线程数\n",
    "\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. 数学操作 Math operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.1 Pointwise Ops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.abs(input, out=None):\\n计算输入张量的每个元素绝对值\\n\\ninput(Tensor) - 输入张量\\nout(Tensor, optional) - 结果张量\\n\\ntorch.ceil(input, out=None):\\n对输入input张量每个元素向上取整，即取不小于每个元素的最小整数，并返回结果到输出\\n\\ntorch.exp(tensor, out=None):\\n返回一个新张量，包含输入input张量每个元素的指数\\n\\ntorch.floor(input, out=None):\\n返回一个新张量，包含输入input张量每个元素的floor，即不大于元素的最大整数。\\n\\ntorch.frac(tensor, out=None):\\n返回每个元素的分数部分\\n\\ntorch.log(input, out=None):\\n计算input的自然对数\\n\\ntorch.log1p(input, out=None):\\n计算input + 1的自然对数y = log(x + 1)\\n对值比较小的输入，此函数比torch.log()更准确\\n\\ntorch.neg(input, out=None):\\n返回一个新张量，包含输入input张量按元素取负。\\n\\ntorch.reciprocal(input, out=None):\\n返回一个新张量，包含输入input张量每个元素的倒数，即1.0/x\\n\\ntorch.round(input, out=None):\\n返回一个新张量，将输入input张量每个元素四舍五入到最近的整数。\\n\\ntorch.rsqrt(input, out=None):\\n返回一个新张量，包含输入input张量每个元素的平方根倒数。\\n\\ntorch.sigmoid(input, out=None):\\n返回一个新张量，包含输入input张量每个元素的sigmoid值\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.abs(input, out=None):\n",
    "计算输入张量的每个元素绝对值\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "out(Tensor, optional) - 结果张量\n",
    "\n",
    "torch.ceil(input, out=None):\n",
    "对输入input张量每个元素向上取整，即取不小于每个元素的最小整数，并返回结果到输出\n",
    "\n",
    "torch.exp(tensor, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的指数\n",
    "\n",
    "torch.floor(input, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的floor，即不大于元素的最大整数。\n",
    "\n",
    "torch.frac(tensor, out=None):\n",
    "返回每个元素的分数部分\n",
    "\n",
    "torch.log(input, out=None):\n",
    "计算input的自然对数\n",
    "\n",
    "torch.log1p(input, out=None):\n",
    "计算input + 1的自然对数y = log(x + 1)\n",
    "对值比较小的输入，此函数比torch.log()更准确\n",
    "\n",
    "torch.neg(input, out=None):\n",
    "返回一个新张量，包含输入input张量按元素取负。\n",
    "\n",
    "torch.reciprocal(input, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的倒数，即1.0/x\n",
    "\n",
    "torch.round(input, out=None):\n",
    "返回一个新张量，将输入input张量每个元素四舍五入到最近的整数。\n",
    "\n",
    "torch.rsqrt(input, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的平方根倒数。\n",
    "\n",
    "torch.sigmoid(input, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的sigmoid值\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(torch.tensor([-1, -3, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.add(input, value, out=None):\\n对输入张量input逐元素加上标量值value，并返回结果到一个新的张量。\\n\\ninput(Tensor) - 输入张量\\nvalue(Number) - 添加到输入每个元素的数\\nout(Tensor, optional)\\n\\ntorch.div(input, value, out=None):\\n将input逐元素除以标量值value，并返回结果到输出张量out\\n\\ntorch.mul(input, value, out=None):\\n用标量值value乘以输入input的每个元素，并返回一个新的结果张量\\n\\ntorch.mul(input, other, out=None):\\n两个张量input, other按元素相乘，并返回到输出张量，两个张量形状不须匹配，但总元素数须一致。当形状不匹配时，input的形状作为输出张量的形状\\n\\ntorch.fmod(input, divisor, out=None):\\n计算除法余数，余数的正负与被除数相同\\n\\ntorch.remainder(input, divisor, out=None):\\n返回一个新张量，包含输入input张量每个元素的除法余数，余数与除数有相同的符号。\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.add(input, value, out=None):\n",
    "对输入张量input逐元素加上标量值value，并返回结果到一个新的张量。\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "value(Number) - 添加到输入每个元素的数\n",
    "out(Tensor, optional)\n",
    "\n",
    "torch.div(input, value, out=None):\n",
    "将input逐元素除以标量值value，并返回结果到输出张量out\n",
    "\n",
    "torch.mul(input, value, out=None):\n",
    "用标量值value乘以输入input的每个元素，并返回一个新的结果张量\n",
    "\n",
    "torch.mul(input, other, out=None):\n",
    "两个张量input, other按元素相乘，并返回到输出张量，两个张量形状不须匹配，但总元素数须一致。当形状不匹配时，input的形状作为输出张量的形状\n",
    "\n",
    "torch.fmod(input, divisor, out=None):\n",
    "计算除法余数，余数的正负与被除数相同\n",
    "\n",
    "torch.remainder(input, divisor, out=None):\n",
    "返回一个新张量，包含输入input张量每个元素的除法余数，余数与除数有相同的符号。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 1., 0., 1.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.addcdiv(tensor, value=1, tensor1, tensor2, out=None):\\n用tensor2对tensor1逐元素相除，然后乘以标量值value并加到tensor上。\\n\\ntensor(Tensor) - 张量\\nvalue(Number, optional) - 标量\\ntensor1(Tensor) - 张量，作为分子\\ntensor2(Tensor) - 张量，作为分母\\nout(Tensor, optional)\\n\\ntorch.addcmul(tensor, value=1, tensor1, tensor2, out=None):\\n用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor，张量形状不需要匹配，但元素数量必须一致。\\n\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None):\n",
    "用tensor2对tensor1逐元素相除，然后乘以标量值value并加到tensor上。\n",
    "\n",
    "tensor(Tensor) - 张量\n",
    "value(Number, optional) - 标量\n",
    "tensor1(Tensor) - 张量，作为分子\n",
    "tensor2(Tensor) - 张量，作为分母\n",
    "out(Tensor, optional)\n",
    "\n",
    "torch.addcmul(tensor, value=1, tensor1, tensor2, out=None):\n",
    "用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor，张量形状不需要匹配，但元素数量必须一致。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3190, 0.1743, 0.1156],\n",
       "        [0.5876, 0.2322, 0.8217],\n",
       "        [0.8436, 0.7516, 0.9999]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(3,3)\n",
    "t1 = torch.rand(3,1)\n",
    "t2 = torch.rand(1,3)\n",
    "torch.addcdiv(t, 0.1, t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.clamp(input, min, max, out=None):\\n将输入input张量每个元素值约束到区间[min, max]，并返回结果到一个新张量\\n也可以只设定min或只设定max\\n\\ninput(Tensor) - 输入张量\\nmin(Number) - 限制范围下限\\nmax(Number) - 限制范围上限\\nout(Tensor, optional)\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.clamp(input, min, max, out=None):\n",
    "将输入input张量每个元素值约束到区间[min, max]，并返回结果到一个新张量\n",
    "也可以只设定min或只设定max\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "min(Number) - 限制范围下限\n",
    "max(Number) - 限制范围上限\n",
    "out(Tensor, optional)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4000, 0.4000, 0.4000],\n",
       "        [0.4000, 0.5085, 0.6000],\n",
       "        [0.5513, 0.6000, 0.6000],\n",
       "        [0.6000, 0.4089, 0.4000],\n",
       "        [0.6000, 0.6000, 0.6000]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(x, 0.4, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.lerp(start, end, weight, out=None):\\n对两个张量以start, end做线性插值，将结果返回到输出张量\\nout = start + weight*(end - start)\\n\\nstart(Tensor) - 起始点张量\\nend(Tensor) - 终止点张量\\nweight(float) - 插值公式中的weight\\nout(Tensor, optional)\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.lerp(start, end, weight, out=None):\n",
    "对两个张量以start, end做线性插值，将结果返回到输出张量\n",
    "out = start + weight*(end - start)\n",
    "\n",
    "start(Tensor) - 起始点张量\n",
    "end(Tensor) - 终止点张量\n",
    "weight(float) - 插值公式中的weight\n",
    "out(Tensor, optional)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 6.0000, 6.5000, 7.0000])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = torch.arange(1., 5.)\n",
    "end = torch.empty(4).fill_(10)\n",
    "torch.lerp(start, end, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.pow(input, exponent, out=None):\\n对输入input按元素求exponent次幂，并返回结果张量。幂可以为float数或与input相同元素数的张量\\n\\ntorch.pow(base, input, out=None):\\nbase为标量浮点值，input为张量。\\n\\nbase(float) - 标量值，指数的底\\ninput(Tensor) - 幂值\\nout(Tensor, optional)\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.pow(input, exponent, out=None):\n",
    "对输入input按元素求exponent次幂，并返回结果张量。幂可以为float数或与input相同元素数的张量\n",
    "\n",
    "torch.pow(base, input, out=None):\n",
    "base为标量浮点值，input为张量。\n",
    "\n",
    "base(float) - 标量值，指数的底\n",
    "input(Tensor) - 幂值\n",
    "out(Tensor, optional)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  8, 16])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(2, torch.arange(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2 Reduction Ops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.dist(input, other, p=2, out=None) -> Tensor:\\n返回(input - other)的p范数\\n\\ninput(Tensor) - 输入张量\\nother(Tensor) - 右侧输入张量\\np(float, optional) - 要计算的范数\\nout(Tensor, optional)\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.dist(input, other, p=2, out=None) -> Tensor:\n",
    "返回(input - other)的p范数\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "other(Tensor) - 右侧输入张量\n",
    "p(float, optional) - 要计算的范数\n",
    "out(Tensor, optional)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6435)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4)\n",
    "b = torch.randn(4)\n",
    "torch.dist(a, b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.cumsum(input, dim, out=None) -> Tensor:\\n返回输入沿指定维度的累积和\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.cumsum(input, dim, out=None) -> Tensor:\n",
    "返回输入沿指定维度的累积和\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7289, -1.1486])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(torch.randn(2), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.dist(input, other, p=2, out=None) -> Tensor:\\n返回(input - other)的p范数\\n\\ninput(Tensor) - 输入张量\\nother(Tensor) - 右侧输入张量\\np(float, optional) - 要计算的范数\\nout(Tensor, optional)\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.dist(input, other, p=2, out=None) -> Tensor:\n",
    "返回(input - other)的p范数\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "other(Tensor) - 右侧输入张量\n",
    "p(float, optional) - 要计算的范数\n",
    "out(Tensor, optional)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2532)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(torch.randn(4), torch.randn(4),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.norm(input, p=2) -> float:\\n返回输入张量input的p范数。\\n\\ninput(Tensor) - 输入张量\\np(float, optional) - 范数计算中的幂指数值\\n\\ntorch.norm(input, p, dim, out=None) -> Tensor:\\n返回输入张量给定维度dim上每行的p范数。\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.norm(input, p=2) -> float:\n",
    "返回输入张量input的p范数。\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "p(float, optional) - 范数计算中的幂指数值\n",
    "\n",
    "torch.norm(input, p, dim, out=None) -> Tensor:\n",
    "返回输入张量给定维度dim上每行的p范数。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8089, 1.6202])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.randn(5,2), 1, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.prod(input) -> float:\\n返回输入张量input所有元素的积\\n\\ntorch.prod(input, dim, out=None) -> Tensor:\\n返回输入张量给定维度上每行的积。\\n\\ntorch.std(input) -> float:\\n返回输入张量input所有元素的标准差\\n\\ntorch.std(input, dim, out=None):\\n返回输入张量给定维度上每行的标准差。\\n\\ntorch.sum(input) -> float:\\n返回输入张量input所有元素的各\\n\\ntorch.sum(input, dim, out=None) -> Tensor:\\n返回输入疑是给定维度上每行的和\\n\\ntorch.var(input) -> float:\\n返回输入张量所有元素的方差\\n\\ntorch.var(input, dim, out=None) -> Tensor:\\n返回输入张量给定维度上每行的方差。\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.prod(input) -> float:\n",
    "返回输入张量input所有元素的积\n",
    "\n",
    "torch.prod(input, dim, out=None) -> Tensor:\n",
    "返回输入张量给定维度上每行的积。\n",
    "\n",
    "torch.std(input) -> float:\n",
    "返回输入张量input所有元素的标准差\n",
    "\n",
    "torch.std(input, dim, out=None):\n",
    "返回输入张量给定维度上每行的标准差。\n",
    "\n",
    "torch.sum(input) -> float:\n",
    "返回输入张量input所有元素的各\n",
    "\n",
    "torch.sum(input, dim, out=None) -> Tensor:\n",
    "返回输入疑是给定维度上每行的和\n",
    "\n",
    "torch.var(input) -> float:\n",
    "返回输入张量所有元素的方差\n",
    "\n",
    "torch.var(input, dim, out=None) -> Tensor:\n",
    "返回输入张量给定维度上每行的方差。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.prod(torch.arange(1., 4.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.3 比较操作Comparison Ops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.eq(input, other, out=None) -> Tensor:\\n比较元素相等性，第二个参数可为一个数，或与第一个参数同类型形状的张量\\n\\ninput(Tensor) - 待比较张量\\nother(Tensor or float) - 比较张量或数\\nout(Tensor, optional) - 输出张量，须为ByteTensor类型或与input同类型\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.eq(input, other, out=None) -> Tensor:\n",
    "比较元素相等性，第二个参数可为一个数，或与第一个参数同类型形状的张量\n",
    "\n",
    "input(Tensor) - 待比较张量\n",
    "other(Tensor or float) - 比较张量或数\n",
    "out(Tensor, optional) - 输出张量，须为ByteTensor类型或与input同类型\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False],\n",
       "        [False,  True]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#若两个张量有相同的形状和元素值，则返回True， 否则False。\n",
    "\n",
    "torch.equal(torch.tensor([[1, 2], [3, 4]]),torch.tensor([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.ge(input, other, out=None) -> Tensor:\\n逐元素比较input和other，即是否input >= other\\n第二个参数可以为一个数或与第一个参数相同形状和类型的张量。\\n\\ninput(Tensor) - 待对比的张量\\nother(Tensor or float) - 对比的张量或float值\\nout(Tensor, optional) - 输出张量，必须为ByteTensor或与第一个参数相同类型。\\n'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.ge(input, other, out=None) -> Tensor:\n",
    "逐元素比较input和other，即是否input >= other\n",
    "第二个参数可以为一个数或与第一个参数相同形状和类型的张量。\n",
    "\n",
    "input(Tensor) - 待对比的张量\n",
    "other(Tensor or float) - 对比的张量或float值\n",
    "out(Tensor, optional) - 输出张量，必须为ByteTensor或与第一个参数相同类型。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True],\n",
       "        [False,  True]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.gt(input, other, out=None) -> Tensor:\\n逐元素比较input和other，是否input > other。若两个张量有相同的形状和元素值，则返回True，否则False。第二个参数\\n可以为一个数或与第一个参数相同形状和类型的张量。\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.gt(input, other, out=None) -> Tensor:\n",
    "逐元素比较input和other，是否input > other。若两个张量有相同的形状和元素值，则返回True，否则False。第二个参数\n",
    "可以为一个数或与第一个参数相同形状和类型的张量。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True],\n",
       "        [False, False]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.kthvalue(input, k, dim=None, out=None) -> (Tensor, LongTensor):\\n取输入张量input指定维度上第k个最小值，若不指定dim，则默认为input的最后一维。返回一个元组，其中indices是原始输入张量input中沿dim维的第k个最小值下标。\\n\\ninput(Tensor) - 输入张量\\nk(int) - 第k个最小值\\ndim(int, optional)` - 沿着此维度进行排序\\nout(tuple, optional) - 输出元组\\n'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.kthvalue(input, k, dim=None, out=None) -> (Tensor, LongTensor):\n",
    "取输入张量input指定维度上第k个最小值，若不指定dim，则默认为input的最后一维。返回一个元组，其中indices是原始输入张量input中沿dim维的第k个最小值下标。\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "k(int) - 第k个最小值\n",
    "dim(int, optional)` - 沿着此维度进行排序\n",
    "out(tuple, optional) - 输出元组\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(\n",
       "values=tensor([0.0182, 0.4338]),\n",
       "indices=tensor([3, 2]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4,2)\n",
    "torch.kthvalue(x, 1, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.le(input, other, out=None) -> Tensor:\\n逐元素比较input和other，即是否input <= other，第二个参数可以为一个数或与第一个参数相同形状和类型的张量。\\n\\ntorch.lt(input, other, out=None) -> Tensor:\\n逐元素比较input和other，即是否input < other\\n\\ntorch.ne(input, other, out=Tensor) -> Tensor:\\n逐元素比较input和other， 即是否input != other。第二个参数可以为一个数或与第一个参数相同形状和类型的张量。\\n返回值：一个torch.ByteTensor张量，包含了每个位置的比较结果(如果tensor != other 为True，返回1)。\\n\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.le(input, other, out=None) -> Tensor:\n",
    "逐元素比较input和other，即是否input <= other，第二个参数可以为一个数或与第一个参数相同形状和类型的张量。\n",
    "\n",
    "torch.lt(input, other, out=None) -> Tensor:\n",
    "逐元素比较input和other，即是否input < other\n",
    "\n",
    "torch.ne(input, other, out=Tensor) -> Tensor:\n",
    "逐元素比较input和other， 即是否input != other。第二个参数可以为一个数或与第一个参数相同形状和类型的张量。\n",
    "返回值：一个torch.ByteTensor张量，包含了每个位置的比较结果(如果tensor != other 为True，返回1)。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True],\n",
       "        [ True, False]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.le(torch.tensor([[1, 2], [3, 4]]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.max(input, dim, max=None, max_indice=None) -> (Tensor, LongTensor):\\n返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。\\n\\ninput(Tensor) - 输入张量\\ndim(int) - 指定的维度\\nmax(Tensor, optional) - 结果张量，包含给定维度上的最大值\\nmax_indices(LongTensor, optional) - 包含给定维度上每个最大值的位置索引。\\n\\ntorch.min(input, dim, min=None, min_indices=None) -> (Tensor, LongTensor):\\n返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引。\\n\\ntorch.min(input, other, out=None) -> Tensor:\\ninput中逐元素与other相应位置的元素对比，返回最小值到输出张量。\\n两张量形状不需匹配，但元素数须相同。\\n'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.max(input, dim, max=None, max_indice=None) -> (Tensor, LongTensor):\n",
    "返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "dim(int) - 指定的维度\n",
    "max(Tensor, optional) - 结果张量，包含给定维度上的最大值\n",
    "max_indices(LongTensor, optional) - 包含给定维度上每个最大值的位置索引。\n",
    "\n",
    "torch.min(input, dim, min=None, min_indices=None) -> (Tensor, LongTensor):\n",
    "返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引。\n",
    "\n",
    "torch.min(input, other, out=None) -> Tensor:\n",
    "input中逐元素与other相应位置的元素对比，返回最小值到输出张量。\n",
    "两张量形状不需匹配，但元素数须相同。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.4699, 1.2911, 0.9563, 0.0776]),\n",
       "indices=tensor([1, 0, 0, 1]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.randn(4, 4), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor):\\n对输入张量input沿着指定维度按升序排序，如果不给定dim，默认为输入的最后一维。如果指定参数descending为True，则按降序排序。\\n返回两项：重排后的张量，和重排后元素在原张量的索引\\n\\ninput(Tensor) - 输入张量\\ndim(int, optional) - 沿此维排序，默认为最后一维\\ndescending(bool, optional) - 布尔值，默认升序\\n\\n'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor):\n",
    "对输入张量input沿着指定维度按升序排序，如果不给定dim，默认为输入的最后一维。如果指定参数descending为True，则按降序排序。\n",
    "返回两项：重排后的张量，和重排后元素在原张量的索引\n",
    "\n",
    "input(Tensor) - 输入张量\n",
    "dim(int, optional) - 沿此维排序，默认为最后一维\n",
    "descending(bool, optional) - 布尔值，默认升序\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[ 0.1528,  1.7705],\n",
       "        [ 0.9364,  1.0923],\n",
       "        [-1.3573,  1.9534]]),\n",
       "indices=tensor([[1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0]]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sort(torch.randn(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor):\\n沿给定dim维度返回输入张量input中k个最大值，不指定dim，则默认为最后一维，如果largest为False，则返回最小的k个值。\\n'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor):\n",
    "沿给定dim维度返回输入张量input中k个最大值，不指定dim，则默认为最后一维，如果largest为False，则返回最小的k个值。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([5., 4.]),\n",
       "indices=tensor([4, 3]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.arange(1., 6.), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4 其它操作 Other Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.cross(input, other, dim=-1, out=None) -> Tensor:\\n返回沿着维度dim上，两个张量input和other的叉积。input和other必须有相同的形状，且指定的dim维上size必须为3。如果不指定dim，则默认为第一个尺度为3的维。\\n'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.cross(input, other, dim=-1, out=None) -> Tensor:\n",
    "返回沿着维度dim上，两个张量input和other的叉积。input和other必须有相同的形状，且指定的dim维上size必须为3。如果不指定dim，则默认为第一个尺度为3的维。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3005, -0.2538, -0.0702],\n",
       "        [-0.0242, -0.1374,  0.1270],\n",
       "        [ 0.4252, -0.2002,  0.0841],\n",
       "        [ 0.3947, -0.0545, -0.0835]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(4, 3)\n",
    "b = torch.rand(4, 3)\n",
    "torch.cross(a, b, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.diag(input, diagonal=0, out=None) -> Tensor:\\n如果输入是一个向量，则返回一个以input为对角线元素的2D方阵\\n如果输入是一个矩阵，则返回一个包含input为对角元素的1D张量\\n参数diagonal指定对角线：\\n\\ndiagonal = 0, 主对角线\\ndiagonal > 0, 主对角线之上\\ndiagonal < 0, 主对角线之下\\n'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.diag(input, diagonal=0, out=None) -> Tensor:\n",
    "如果输入是一个向量，则返回一个以input为对角线元素的2D方阵\n",
    "如果输入是一个矩阵，则返回一个包含input为对角元素的1D张量\n",
    "参数diagonal指定对角线：\n",
    "\n",
    "diagonal = 0, 主对角线\n",
    "diagonal > 0, 主对角线之上\n",
    "diagonal < 0, 主对角线之下\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5010, 0.5550, 0.8334])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(a, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.5 BLAS and LAPACK Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算两个张量的点乘，两个张量都为1-D向量\n",
    "\n",
    "torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.eig(a, eigenvectors=False, out=None) -> (Tensor, Tensor):\\n计算方阵a的特征值和特征向量。\\n\\na(Tensor) - 方阵\\neigenvectors(bool) - 如果为True，同时计算特征值和特征微量，否则只计算特征值\\n返回值：\\ne(Tensor) - a的右特征向量\\nv(Tensor) - 如果eigenvectors为True，则为包含特征向量的张量，否则为空。\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.eig(a, eigenvectors=False, out=None) -> (Tensor, Tensor):\n",
    "计算方阵a的特征值和特征向量。\n",
    "\n",
    "a(Tensor) - 方阵\n",
    "eigenvectors(bool) - 如果为True，同时计算特征值和特征微量，否则只计算特征值\n",
    "返回值：\n",
    "e(Tensor) - a的右特征向量\n",
    "v(Tensor) - 如果eigenvectors为True，则为包含特征向量的张量，否则为空。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[-0.5800,  0.0000],\n",
       "        [ 0.4666,  1.2305],\n",
       "        [ 0.4666, -1.2305],\n",
       "        [ 2.9537,  0.0000]]),\n",
       "eigenvectors=tensor([[ 0.6441, -0.1876, -0.4172,  0.1589],\n",
       "        [-0.3511, -0.1679, -0.1145,  0.5574],\n",
       "        [ 0.3840,  0.7927,  0.0000,  0.1068],\n",
       "        [ 0.5607,  0.2618,  0.2293,  0.8079]]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "torch.eig(a, eigenvectors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6081,  1.0322, -0.1980, -0.4998],\n",
       "        [ 0.3066, -0.6002, -0.4268,  0.6438],\n",
       "        [ 0.2513,  1.5589,  1.0621, -1.2205],\n",
       "        [-0.5015,  1.5097,  0.7268, -0.7004]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对方阵input求逆\n",
    "torch.inverse(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4574, 0.8024, 1.0718, 0.3288, 0.5557],\n",
       "        [0.4101, 0.4200, 0.6404, 0.6583, 0.2505]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对矩阵mat1和mat2进行相乘。\n",
    "x = torch.rand(2, 3)\n",
    "y = torch.rand(3, 5)\n",
    "torch.mm(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2337, 0.8232])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对矩阵mat和向量vec进行相乘。\n",
    "v = torch.rand(3)\n",
    "torch.mv(x, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spinningup] *",
   "language": "python",
   "name": "conda-env-spinningup-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
